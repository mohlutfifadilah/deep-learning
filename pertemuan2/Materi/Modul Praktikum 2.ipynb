{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5fc55b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">KLASIFIKASI 2 KELAS</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aea2ef",
   "metadata": {},
   "source": [
    "### Activation pada Dense\n",
    "\n",
    "Setelah kita melakukan regresi menggunakan activation `relu` dan `linear`, pada pertemuan kali ini, kita akan mencoba melakukan klasifikasi menggunakan Dense dengan 2 jenis activation yang berbeda yaitu\n",
    "\n",
    "1.   `sigmoid`\n",
    "2.   `softmax`\n",
    "\n",
    "Jika `linear` kita pergunakan untuk output yang berupa `float`, maka `sigmoid` dan `softmax` kita pergunakan untuk kasus klasifikasi dengan hasil dalam bentuk persentase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749f6a5",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "`sigmoid` kita pergunakan jika kelas yang kita klasifikasikan/kategorikan hanya berjumlah 2 kelas saja. Istilah lainnya untuk kasus ini adalah kasus biner atau binary. Contohnya, kelas `ya` dan `tidak`, atau kelas `cowok` dan `cewek`, atau kelas `sehat` dan `sakit`, dan seterusnya. Tentu saja, kelas-kelas ini haruslah kita representasikan dalam bentuk `0` dan `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21576bc8",
   "metadata": {},
   "source": [
    "# Persiapan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a3395b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.633563</td>\n",
       "      <td>0.357385</td>\n",
       "      <td>-0.503931</td>\n",
       "      <td>0.935066</td>\n",
       "      <td>0.647981</td>\n",
       "      <td>-0.050796</td>\n",
       "      <td>-1.933989</td>\n",
       "      <td>2.081684</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>-0.258298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.283905</td>\n",
       "      <td>1.109459</td>\n",
       "      <td>-0.908953</td>\n",
       "      <td>1.006586</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>1.107295</td>\n",
       "      <td>1.243526</td>\n",
       "      <td>-0.172200</td>\n",
       "      <td>1.150359</td>\n",
       "      <td>0.147744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.966476</td>\n",
       "      <td>-0.593314</td>\n",
       "      <td>0.458020</td>\n",
       "      <td>1.032323</td>\n",
       "      <td>1.283685</td>\n",
       "      <td>-0.317640</td>\n",
       "      <td>1.499045</td>\n",
       "      <td>0.434477</td>\n",
       "      <td>0.423678</td>\n",
       "      <td>1.251380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.429309</td>\n",
       "      <td>-1.306530</td>\n",
       "      <td>-1.869925</td>\n",
       "      <td>3.092164</td>\n",
       "      <td>2.028800</td>\n",
       "      <td>-0.879635</td>\n",
       "      <td>-0.393494</td>\n",
       "      <td>-0.101213</td>\n",
       "      <td>-1.624066</td>\n",
       "      <td>0.443553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.204798</td>\n",
       "      <td>0.078464</td>\n",
       "      <td>0.705181</td>\n",
       "      <td>0.224765</td>\n",
       "      <td>0.618707</td>\n",
       "      <td>1.534946</td>\n",
       "      <td>-0.302288</td>\n",
       "      <td>2.325055</td>\n",
       "      <td>0.495505</td>\n",
       "      <td>0.538133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-1.606404</td>\n",
       "      <td>0.228927</td>\n",
       "      <td>0.959690</td>\n",
       "      <td>0.145821</td>\n",
       "      <td>0.682755</td>\n",
       "      <td>-0.927143</td>\n",
       "      <td>-0.280438</td>\n",
       "      <td>0.789222</td>\n",
       "      <td>-1.330100</td>\n",
       "      <td>-1.463687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.154048</td>\n",
       "      <td>-0.120265</td>\n",
       "      <td>-0.643621</td>\n",
       "      <td>-0.467386</td>\n",
       "      <td>-0.825604</td>\n",
       "      <td>0.725398</td>\n",
       "      <td>-1.439272</td>\n",
       "      <td>-1.132146</td>\n",
       "      <td>1.511610</td>\n",
       "      <td>-0.114986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.841468</td>\n",
       "      <td>-0.593749</td>\n",
       "      <td>-0.391671</td>\n",
       "      <td>-0.955036</td>\n",
       "      <td>-1.169619</td>\n",
       "      <td>0.683856</td>\n",
       "      <td>-1.629486</td>\n",
       "      <td>0.289335</td>\n",
       "      <td>-0.434358</td>\n",
       "      <td>-1.271335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.080252</td>\n",
       "      <td>-0.607761</td>\n",
       "      <td>-0.488605</td>\n",
       "      <td>-1.338506</td>\n",
       "      <td>-1.605447</td>\n",
       "      <td>-1.689724</td>\n",
       "      <td>0.202908</td>\n",
       "      <td>0.291496</td>\n",
       "      <td>0.827980</td>\n",
       "      <td>-1.069399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-1.205338</td>\n",
       "      <td>-1.719647</td>\n",
       "      <td>0.994358</td>\n",
       "      <td>-2.060993</td>\n",
       "      <td>-1.493417</td>\n",
       "      <td>-0.664010</td>\n",
       "      <td>0.718480</td>\n",
       "      <td>-0.305819</td>\n",
       "      <td>-1.405728</td>\n",
       "      <td>-0.703207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0     0.633563  0.357385 -0.503931  0.935066  0.647981 -0.050796 -1.933989   \n",
       "1     1.283905  1.109459 -0.908953  1.006586  0.492219  1.107295  1.243526   \n",
       "2    -0.966476 -0.593314  0.458020  1.032323  1.283685 -0.317640  1.499045   \n",
       "3     2.429309 -1.306530 -1.869925  3.092164  2.028800 -0.879635 -0.393494   \n",
       "4    -1.204798  0.078464  0.705181  0.224765  0.618707  1.534946 -0.302288   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995 -1.606404  0.228927  0.959690  0.145821  0.682755 -0.927143 -0.280438   \n",
       "9996  1.154048 -0.120265 -0.643621 -0.467386 -0.825604  0.725398 -1.439272   \n",
       "9997  0.841468 -0.593749 -0.391671 -0.955036 -1.169619  0.683856 -1.629486   \n",
       "9998  1.080252 -0.607761 -0.488605 -1.338506 -1.605447 -1.689724  0.202908   \n",
       "9999 -1.205338 -1.719647  0.994358 -2.060993 -1.493417 -0.664010  0.718480   \n",
       "\n",
       "            x8        x9       x10  y  \n",
       "0     2.081684  0.041266 -0.258298  1  \n",
       "1    -0.172200  1.150359  0.147744  1  \n",
       "2     0.434477  0.423678  1.251380  1  \n",
       "3    -0.101213 -1.624066  0.443553  1  \n",
       "4     2.325055  0.495505  0.538133  0  \n",
       "...        ...       ...       ... ..  \n",
       "9995  0.789222 -1.330100 -1.463687  0  \n",
       "9996 -1.132146  1.511610 -0.114986  0  \n",
       "9997  0.289335 -0.434358 -1.271335  0  \n",
       "9998  0.291496  0.827980 -1.069399  0  \n",
       "9999 -0.305819 -1.405728 -0.703207  0  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Dataset/dataset_klasifikasi_biner.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15b84c",
   "metadata": {},
   "source": [
    "# Membagi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea14d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 10), (2000, 10), (8000,), (2000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop('y', axis=1).values\n",
    "y = df['y'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df030462",
   "metadata": {},
   "source": [
    "# Membuat Arsitektur Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b0c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 15:31:55.389278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f98eb",
   "metadata": {},
   "source": [
    "Kemudian mari kita siapkan JST dengan susunan `32-16-8-4-2` dan `1`. Perhatikan, pada layer terakhir kita beri nilai `Dense`-nya `1` dengan activation `sigmoid`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f988cc1",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6288a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                352       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1065 (4.16 KB)\n",
      "Trainable params: 1065 (4.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640eb39",
   "metadata": {},
   "source": [
    "Untuk kasus klasifikasi biner, kita bisa pergunakan `loss`-nya `binary_crossentropy`. Kemudian, selain `loss`, pada praktikum kali ini kita tambahkan satu lagi variabel `metrics` dengan isian `'accuracy'`, untuk mengetahui seberapa akurat otak kita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a549",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f98b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 5s 44ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 7.6643 - accuracy: 0.5031 - val_loss: 8.0017 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "catatan = model.fit(x_train, y_train, validation_split=0.2, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64d260",
   "metadata": {},
   "source": [
    "# Plot Performa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6156c5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(catatan.history['val_accuracy'])), catatan.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(range(len(catatan.history['accuracy'])), catatan.history['accuracy'], label='accuracy')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcc9a6",
   "metadata": {},
   "source": [
    "# Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7553c1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 7.7279 - accuracy: 0.4990\n",
      "Accuracy Model :  0.49900001287460327\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('Accuracy Model : ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67853fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cda01",
   "metadata": {},
   "source": [
    "# Prediksi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd3c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf1413",
   "metadata": {},
   "source": [
    "Untuk menormalisasikan ke dalam bentuk `0` dan `1`, maka kita perlu menambahkan baris kode sebagai berikut, yang intinya untuk segala nilai yang di atas $0.5$ akan kita rubah ke `kelas 1` dan untuk segala nilai yang di bawah $0.5$ akan kita rubah ke `kelas 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a572bacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = (pred > 0.5) * 1\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262ca0e",
   "metadata": {},
   "source": [
    "# Evaluasi dengan Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381d28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 998 1002]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
